{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT Finetuning.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "70842f69b3f04a82835d5afa8ccf5a8c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1c03acc0a7fc4a0ab3baeedc87be931b",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_57767b6fca89415b84c77c1b7cb54d12",
              "IPY_MODEL_39e7748a61e74356b709b065b0b3bd02"
            ]
          }
        },
        "1c03acc0a7fc4a0ab3baeedc87be931b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "57767b6fca89415b84c77c1b7cb54d12": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_9f5653c0e5be4254ac9a159a295fe0df",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 231508,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 231508,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9a32ddaf67cd4a8e92ff608a955caa40"
          }
        },
        "39e7748a61e74356b709b065b0b3bd02": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_9792c84aaf894d46ad9b03646a8ece40",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 232k/232k [00:00&lt;00:00, 298kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_bcce7fde03334b4ebdeb574150dd8c39"
          }
        },
        "9f5653c0e5be4254ac9a159a295fe0df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9a32ddaf67cd4a8e92ff608a955caa40": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "9792c84aaf894d46ad9b03646a8ece40": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "bcce7fde03334b4ebdeb574150dd8c39": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "0267482e99034b5e8560ecead0d8ea18": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e6090c4cbdcf4d3aa013c7eaf6b30b90",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_253b469015d14e36b04ef84e412689f0",
              "IPY_MODEL_0f0fdb2de5e64edab1e7ccf52d0a1a2e"
            ]
          }
        },
        "e6090c4cbdcf4d3aa013c7eaf6b30b90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "253b469015d14e36b04ef84e412689f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_a5411ccee4d34320a6bc7be8544c0f20",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 433,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 433,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_414149eac1e647d4a9ba87b6d143441c"
          }
        },
        "0f0fdb2de5e64edab1e7ccf52d0a1a2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_de48296600154765a0ab448e02f29191",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 433/433 [00:00&lt;00:00, 1.11kB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_aba6eb7e4df54e54a3511e3fde58bde4"
          }
        },
        "a5411ccee4d34320a6bc7be8544c0f20": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "414149eac1e647d4a9ba87b6d143441c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "de48296600154765a0ab448e02f29191": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "aba6eb7e4df54e54a3511e3fde58bde4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c695fed3cfae4ad8803439f481cf7baa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_c227d3aa279e49fba07d0edd77bc4db1",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_38cd350858a443dc99b0a6a303d11ae0",
              "IPY_MODEL_41e9912381734c9f8d423b814e3282dd"
            ]
          }
        },
        "c227d3aa279e49fba07d0edd77bc4db1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "38cd350858a443dc99b0a6a303d11ae0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_23ca4c8f5ddc4f5da6807cbc97a3f0d9",
            "_dom_classes": [],
            "description": "Downloading: 100%",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 440473133,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 440473133,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_2abbbccead944ad7a0b0bbb952ea4f6d"
          }
        },
        "41e9912381734c9f8d423b814e3282dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_159743ceaada4716950731bba2ac71c7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 440M/440M [00:10&lt;00:00, 43.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_262ef65363054f23b296e568113ba5f1"
          }
        },
        "23ca4c8f5ddc4f5da6807cbc97a3f0d9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "2abbbccead944ad7a0b0bbb952ea4f6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "159743ceaada4716950731bba2ac71c7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "262ef65363054f23b296e568113ba5f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "ttIWDazyRfkK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f81a3a5b-d014-4d14-942f-c8e44beecc49"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rjnD9AMFRvHj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "79f504b4-c315-4ce8-ae58-9f4b187d4b64"
      },
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla K80\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7MqXc8CvR0LA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 595
        },
        "outputId": "f8a9187e-dbb1-4ee8-929f-bb72a89a40c5"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting transformers\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/27/3c/91ed8f5c4e7ef3227b4119200fc0ed4b4fd965b1f0172021c25701087825/transformers-3.0.2-py3-none-any.whl (769kB)\n",
            "\u001b[K     |████████████████████████████████| 778kB 2.7MB/s \n",
            "\u001b[?25hCollecting sentencepiece!=0.1.92\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/a4/d0a884c4300004a78cca907a6ff9a5e9fe4f090f5d95ab341c53d28cbc58/sentencepiece-0.1.91-cp36-cp36m-manylinux1_x86_64.whl (1.1MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1MB 13.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.7)\n",
            "Collecting tokenizers==0.8.1.rc1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/40/d0/30d5f8d221a0ed981a186c8eb986ce1c94e3a6e87f994eae9f4aa5250217/tokenizers-0.8.1rc1-cp36-cp36m-manylinux1_x86_64.whl (3.0MB)\n",
            "\u001b[K     |████████████████████████████████| 3.0MB 13.9MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.18.5)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Collecting sacremoses\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/7d/34/09d19aff26edcc8eb2a01bed8e98f13a1537005d31e95233fd48216eed10/sacremoses-0.0.43.tar.gz (883kB)\n",
            "\u001b[K     |████████████████████████████████| 890kB 38.2MB/s \n",
            "\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (1.15.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (0.16.0)\n",
            "Building wheels for collected packages: sacremoses\n",
            "  Building wheel for sacremoses (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for sacremoses: filename=sacremoses-0.0.43-cp36-none-any.whl size=893260 sha256=849bbc7d84c5d572b920f16f24c16c7e35ea54359c6db0b16db24fd408c54064\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/3c/fd/7ce5c3f0666dab31a50123635e6fb5e19ceb42ce38d4e58f45\n",
            "Successfully built sacremoses\n",
            "Installing collected packages: sentencepiece, tokenizers, sacremoses, transformers\n",
            "Successfully installed sacremoses-0.0.43 sentencepiece-0.1.91 tokenizers-0.8.1rc1 transformers-3.0.2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F8k3mEtSTmX-",
        "colab_type": "code",
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "4e60a505-bb51-499a-a742-d08818b8ce43"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-b6771241-25fc-4be1-8981-bb3056c6c68e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-b6771241-25fc-4be1-8981-bb3056c6c68e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "Saving messages.csv to messages.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1flJy3ISUJ23",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "807fb213-ab76-4f14-9439-ce0c9c91f59d"
      },
      "source": [
        "import io\n",
        "import pandas as pd\n",
        "df = pd.read_csv(io.BytesIO(uploaded['messages.csv']))\n",
        "# Dataset is now stored in a Pandas Dataframe\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>message_id</th>\n",
              "      <th>response_id</th>\n",
              "      <th>article_id</th>\n",
              "      <th>empathy</th>\n",
              "      <th>distress</th>\n",
              "      <th>empathy_bin</th>\n",
              "      <th>distress_bin</th>\n",
              "      <th>essay</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>R_1hGrPtWM4SumG0U_1</td>\n",
              "      <td>R_1hGrPtWM4SumG0U</td>\n",
              "      <td>67</td>\n",
              "      <td>5.667</td>\n",
              "      <td>4.375</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>it is really diheartening to read about these ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>R_1hGrPtWM4SumG0U_2</td>\n",
              "      <td>R_1hGrPtWM4SumG0U</td>\n",
              "      <td>86</td>\n",
              "      <td>4.833</td>\n",
              "      <td>4.875</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the phone lines from the suicide prevention li...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>R_1hGrPtWM4SumG0U_3</td>\n",
              "      <td>R_1hGrPtWM4SumG0U</td>\n",
              "      <td>206</td>\n",
              "      <td>5.333</td>\n",
              "      <td>3.500</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>no matter what your heritage, you should be ab...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>R_1hGrPtWM4SumG0U_4</td>\n",
              "      <td>R_1hGrPtWM4SumG0U</td>\n",
              "      <td>290</td>\n",
              "      <td>4.167</td>\n",
              "      <td>5.250</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>it is frightening to learn about all these sha...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>R_1hGrPtWM4SumG0U_5</td>\n",
              "      <td>R_1hGrPtWM4SumG0U</td>\n",
              "      <td>342</td>\n",
              "      <td>5.333</td>\n",
              "      <td>4.625</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>the eldest generation of russians aren't being...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "            message_id  ...                                              essay\n",
              "0  R_1hGrPtWM4SumG0U_1  ...  it is really diheartening to read about these ...\n",
              "1  R_1hGrPtWM4SumG0U_2  ...  the phone lines from the suicide prevention li...\n",
              "2  R_1hGrPtWM4SumG0U_3  ...  no matter what your heritage, you should be ab...\n",
              "3  R_1hGrPtWM4SumG0U_4  ...  it is frightening to learn about all these sha...\n",
              "4  R_1hGrPtWM4SumG0U_5  ...  the eldest generation of russians aren't being...\n",
              "\n",
              "[5 rows x 8 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bl4Hl7OAUd0D",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "df = df[['essay', 'empathy', 'empathy_bin']]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p_7goVBnUvZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "edb81e4c-6aa3-42ac-d1ab-6f161a28f5c4"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "df = shuffle(df, random_state=103).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>essay</th>\n",
              "      <th>empathy</th>\n",
              "      <th>empathy_bin</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>I can't really believe he was found dead in ol...</td>\n",
              "      <td>3.500</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>This is just disgusting. I cannot believe that...</td>\n",
              "      <td>1.000</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>I just read a story about a study being done b...</td>\n",
              "      <td>7.000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>GBR The great barrier reef. I read an article ...</td>\n",
              "      <td>6.500</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>There was another gunman attacking random peop...</td>\n",
              "      <td>4.333</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                               essay  empathy  empathy_bin\n",
              "0  I can't really believe he was found dead in ol...    3.500            0\n",
              "1  This is just disgusting. I cannot believe that...    1.000            0\n",
              "2  I just read a story about a study being done b...    7.000            1\n",
              "3  GBR The great barrier reef. I read an article ...    6.500            1\n",
              "4  There was another gunman attacking random peop...    4.333            1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6sS68OVLVRJg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "823c319a-ebbb-426c-fe0f-3c0308984bb9"
      },
      "source": [
        "train = df[:1760]\n",
        "test = df[1760:]\n",
        "print(train.shape)\n",
        "print(test.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1760, 3)\n",
            "(100, 3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7kmjtdbJVphK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "afb027ca-b0d4-4c7d-b57c-075eca0db8d0"
      },
      "source": [
        "train.groupby('empathy_bin').size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "empathy_bin\n",
              "0    894\n",
              "1    866\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9QOXcNPbV9fQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "80fb76a0-e1a8-45ee-83b0-d63200894828"
      },
      "source": [
        "test.groupby('empathy_bin').size()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "empathy_bin\n",
              "0    50\n",
              "1    50\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uxG_blc6WAcF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 83,
          "referenced_widgets": [
            "70842f69b3f04a82835d5afa8ccf5a8c",
            "1c03acc0a7fc4a0ab3baeedc87be931b",
            "57767b6fca89415b84c77c1b7cb54d12",
            "39e7748a61e74356b709b065b0b3bd02",
            "9f5653c0e5be4254ac9a159a295fe0df",
            "9a32ddaf67cd4a8e92ff608a955caa40",
            "9792c84aaf894d46ad9b03646a8ece40",
            "bcce7fde03334b4ebdeb574150dd8c39"
          ]
        },
        "outputId": "6db09d2c-38ce-4749-8da4-568bd1cdb7ad"
      },
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading BERT tokenizer...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "70842f69b3f04a82835d5afa8ccf5a8c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=231508.0, style=ProgressStyle(descripti…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZozR08_WKKA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "098d59a0-36cb-4a63-988f-2b42e0ae475c"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "input_ids = []\n",
        "\n",
        "# Record the length of each sequence (after truncating to 512).\n",
        "lengths = []\n",
        "\n",
        "print('Tokenizing comments...')\n",
        "\n",
        "# For every sentence...\n",
        "for sen in train.essay:\n",
        "    \n",
        "    # Report progress.\n",
        "    if ((len(input_ids) % 20000) == 0):\n",
        "        print('  Read {:,} comments.'.format(len(input_ids)))\n",
        "    \n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sen,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        #max_length = 512,          # Truncate all sentences.                        \n",
        "                        #return_tensors = 'pt',     # Return pytorch tensors.\n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    input_ids.append(encoded_sent)\n",
        "\n",
        "    # Record the truncated length.\n",
        "    lengths.append(len(encoded_sent))\n",
        "\n",
        "print('DONE.')\n",
        "print('{:>10,} comments'.format(len(input_ids)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Tokenizing comments...\n",
            "  Read 0 comments.\n",
            "DONE.\n",
            "     1,760 comments\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B1UcWOXiWk3U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ea1d0163-eb2f-418b-f525-a6890d00f06f"
      },
      "source": [
        "# Also retrieve the labels as a list.\n",
        "\n",
        "# Get the labels from the DataFrame, and convert from booleans to ints.\n",
        "labels = train.empathy_bin.to_numpy().astype(int)\n",
        "\n",
        "print('{:>7,} empathetic'.format(np.sum(labels)))\n",
        "print('{:>7,} not empathetic'.format(len(labels) - np.sum(labels)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "    866 empathetic\n",
            "    894 not empathetic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5qL08YlFXGza",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        },
        "outputId": "b7724295-8a24-4609-8512-77164857b830"
      },
      "source": [
        "print('   Min length: {:,} tokens'.format(min(lengths)))\n",
        "print('   Max length: {:,} tokens'.format(max(lengths)))\n",
        "print('Median length: {:,} tokens'.format(np.median(lengths)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   Min length: 57 tokens\n",
            "   Max length: 201 tokens\n",
            "Median length: 88.0 tokens\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eZGufauyXMdI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        },
        "outputId": "38d7ed3c-810e-4567-9a44-171b2c7d48ce"
      },
      "source": [
        "# We'll borrow the `pad_sequences` utility function to do this.\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Set the required sequence length.\n",
        "MAX_LEN = 256\n",
        "\n",
        "print('\\nPadding/truncating all sentences to %d values...' % MAX_LEN)\n",
        "\n",
        "print('\\nPadding token: \"{:}\", ID: {:}'.format(tokenizer.pad_token, tokenizer.pad_token_id))\n",
        "\n",
        "# Pad our input tokens with value 0.\n",
        "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", \n",
        "                          value=0, truncating=\"post\", padding=\"post\")\n",
        "\n",
        "print('\\nDone.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "Padding/truncating all sentences to 256 values...\n",
            "\n",
            "Padding token: \"[PAD]\", ID: 0\n",
            "\n",
            "Done.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h47iFDA0Xq_F",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create attention masks\n",
        "attention_masks = []\n",
        "\n",
        "# For each sentence...\n",
        "for sent in input_ids:\n",
        "    \n",
        "    # Create the attention mask.\n",
        "    #   - If a token ID is 0, then it's padding, set the mask to 0.\n",
        "    #   - If a token ID is > 0, then it's a real token, set the mask to 1.\n",
        "    att_mask = [int(token_id > 0) for token_id in sent]\n",
        "    \n",
        "    # Store the attention mask for this sentence.\n",
        "    attention_masks.append(att_mask)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgCnAz6AXvc4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Use train_test_split to split our data into train and validation sets for\n",
        "# training\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Use 90% for training and 10% for validation.\n",
        "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels, \n",
        "                                                            random_state=2018, test_size=0.1)\n",
        "# Do the same for the masks.\n",
        "train_masks, validation_masks, _, _ = train_test_split(attention_masks, labels,\n",
        "                                             random_state=2018, test_size=0.1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8cXV2GjpYGrE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Convert all inputs and labels into torch tensors, the required datatype \n",
        "# for our model.\n",
        "train_inputs = torch.tensor(train_inputs)\n",
        "validation_inputs = torch.tensor(validation_inputs)\n",
        "\n",
        "train_labels = torch.tensor(train_labels)\n",
        "validation_labels = torch.tensor(validation_labels)\n",
        "\n",
        "train_masks = torch.tensor(train_masks)\n",
        "validation_masks = torch.tensor(validation_masks)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwPpj8ZFYO5k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "# The DataLoader needs to know our batch size for training, so we specify it \n",
        "# here.\n",
        "# For fine-tuning BERT on a specific task, the authors recommend a batch size of\n",
        "# 16 or 32.\n",
        "\n",
        "batch_size = 32\n",
        "\n",
        "# Create the DataLoader for our training set.\n",
        "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
        "\n",
        "# Create the DataLoader for our validation set.\n",
        "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
        "validation_sampler = SequentialSampler(validation_data)\n",
        "validation_dataloader = DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xcQKqXMiYkFI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0267482e99034b5e8560ecead0d8ea18",
            "e6090c4cbdcf4d3aa013c7eaf6b30b90",
            "253b469015d14e36b04ef84e412689f0",
            "0f0fdb2de5e64edab1e7ccf52d0a1a2e",
            "a5411ccee4d34320a6bc7be8544c0f20",
            "414149eac1e647d4a9ba87b6d143441c",
            "de48296600154765a0ab448e02f29191",
            "aba6eb7e4df54e54a3511e3fde58bde4",
            "c695fed3cfae4ad8803439f481cf7baa",
            "c227d3aa279e49fba07d0edd77bc4db1",
            "38cd350858a443dc99b0a6a303d11ae0",
            "41e9912381734c9f8d423b814e3282dd",
            "23ca4c8f5ddc4f5da6807cbc97a3f0d9",
            "2abbbccead944ad7a0b0bbb952ea4f6d",
            "159743ceaada4716950731bba2ac71c7",
            "262ef65363054f23b296e568113ba5f1"
          ]
        },
        "outputId": "2140e1ed-3c77-41b9-ae8a-a2e9d77cba1c"
      },
      "source": [
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "# Load BertForSequenceClassification, the pretrained BERT model with a single \n",
        "# linear classification layer on top. \n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "    \"bert-base-uncased\",\n",
        "    num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "                    # You can increase this for multi-class tasks.   \n",
        "    output_attentions = False, # Whether the model returns attentions weights.\n",
        "    output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU.\n",
        "model.cuda()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0267482e99034b5e8560ecead0d8ea18",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=433.0, style=ProgressStyle(description_…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "c695fed3cfae4ad8803439f481cf7baa",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(FloatProgress(value=0.0, description='Downloading', max=440473133.0, style=ProgressStyle(descri…"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPretraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eLrAdAITYr_g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "# Note: AdamW is a class from the huggingface library (as opposed to pytorch) \n",
        "# I believe the 'W' stands for 'Weight Decay fix\"\n",
        "optimizer = AdamW(model.parameters(),\n",
        "                  lr = 2e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rZaOAKmGY1iO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "# Number of training epochs (authors recommend between 2 and 4)\n",
        "epochs = 12\n",
        "\n",
        "# Total number of training steps is number of batches * number of epochs.\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P2uyRISjZAhP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vv45bMQzZIjw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ss8jZahgZKqs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "26a468f8-f489-4201-cc56-4f07bdefe664"
      },
      "source": [
        "import random\n",
        "\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "# Store the average loss after each epoch so we can plot them.\n",
        "loss_values = []\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Progress update every 100 batches.\n",
        "        if step % 100 == 0 and not step == 0:\n",
        "            # Calculate elapsed time in minutes.\n",
        "            elapsed = format_time(time.time() - t0)\n",
        "            \n",
        "            # Report progress.\n",
        "            print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        #b_input_ids = batch[0]\n",
        "        #b_input_mask = batch[1]\n",
        "        #b_labels = batch[2]\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # This will return the loss (rather than the model output) because we\n",
        "        # have provided the `labels`.\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        outputs = model(b_input_ids, \n",
        "                    token_type_ids=None, \n",
        "                    attention_mask=b_input_mask, \n",
        "                    labels=b_labels)\n",
        "        \n",
        "        # The call to `model` always returns a tuple, so we need to pull the \n",
        "        # loss value out of the tuple.\n",
        "        loss = outputs[0]\n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over the training data.\n",
        "    avg_train_loss = total_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Store the loss value for plotting the learning curve.\n",
        "    loss_values.append(avg_train_loss)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epcoh took: {:}\".format(format_time(time.time() - t0)))\n",
        "        \n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    eval_loss, eval_accuracy = 0, 0\n",
        "    nb_eval_steps, nb_eval_examples = 0, 0\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in validation_dataloader:\n",
        "        \n",
        "        # Add batch to GPU\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        #batch = tuple(t for t in batch)\n",
        "        \n",
        "        # Unpack the inputs from our dataloader\n",
        "        b_input_ids, b_input_mask, b_labels = batch\n",
        "        \n",
        "        # Telling the model not to compute or store gradients, saving memory and\n",
        "        # speeding up validation\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # This will return the logits rather than the loss because we have\n",
        "            # not provided labels.\n",
        "            # token_type_ids is the same as the \"segment ids\", which \n",
        "            # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "            # The documentation for this `model` function is here: \n",
        "            # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "            outputs = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask)\n",
        "        \n",
        "        # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "        # values prior to applying an activation function like the softmax.\n",
        "        logits = outputs[0]\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        \n",
        "        # Calculate the accuracy for this batch of test sentences.\n",
        "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
        "        \n",
        "        # Accumulate the total accuracy.\n",
        "        eval_accuracy += tmp_eval_accuracy\n",
        "\n",
        "        # Track the number of batches\n",
        "        nb_eval_steps += 1\n",
        "\n",
        "    # Report the final accuracy for this validation run.\n",
        "    print(\"  Accuracy: {0:.2f}\".format(eval_accuracy/nb_eval_steps))\n",
        "    print(\"  Validation took: {:}\".format(format_time(time.time() - t0)))\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "======== Epoch 1 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.68\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 2 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.59\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 3 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.64\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 4 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 5 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.22\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 6 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 7 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 8 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.06\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.63\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 9 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 10 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 11 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:58\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "======== Epoch 12 / 12 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:01:57\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.61\n",
            "  Validation took: 0:00:05\n",
            "\n",
            "Training complete!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYFrcXyXZPld",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 461
        },
        "outputId": "af868088-3436-4b0c-8e03-7f937c6f8f5b"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "% matplotlib inline\n",
        "\n",
        "import seaborn as sns\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(loss_values, 'b-o')\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 864x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-JF8_pfWagTu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "deafef10-1ea4-46ce-be2d-8c6ad61b66a4"
      },
      "source": [
        "# Tokenize all of the sentences and map the tokens to thier word IDs.\n",
        "test_input_ids = []\n",
        "\n",
        "# For every sentence...\n",
        "for sen in test.essay:\n",
        "    \n",
        "    # Report progress.\n",
        "    if ((len(input_ids) % 20000) == 0):\n",
        "        print('  Read {:,} comments.'.format(len(input_ids)))\n",
        "    \n",
        "    # `encode` will:\n",
        "    #   (1) Tokenize the sentence.\n",
        "    #   (2) Prepend the `[CLS]` token to the start.\n",
        "    #   (3) Append the `[SEP]` token to the end.\n",
        "    #   (4) Map tokens to their IDs.\n",
        "    encoded_sent = tokenizer.encode(\n",
        "                        sen,                      # Sentence to encode.\n",
        "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = MAX_LEN,          # Truncate all sentences.                        \n",
        "                   )\n",
        "    \n",
        "    # Add the encoded sentence to the list.\n",
        "    test_input_ids.append(encoded_sent)\n",
        "\n",
        "print('DONE.')\n",
        "print('')\n",
        "print('{:>10,} test comments'.format(len(test_input_ids)))\n",
        "\n",
        "# Also retrieve the labels as a list.\n",
        "\n",
        "# Get the labels from the DataFrame, and convert from booleans to ints.\n",
        "test_labels = test.empathy_bin.to_numpy().astype(int)\n",
        "\n",
        "print('{:>10,} empethatic'.format(np.sum(test_labels)))\n",
        "print('{:>10,} not empetgatic'.format(len(test_labels) - np.sum(test_labels)))\n",
        "\n",
        "# Pad our input tokens\n",
        "test_input_ids = pad_sequences(test_input_ids, maxlen=MAX_LEN, \n",
        "                               dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
        "\n",
        "# Create attention masks\n",
        "test_attention_masks = []\n",
        "\n",
        "# Create a mask of 1s for each token followed by 0s for padding\n",
        "for seq in test_input_ids:\n",
        "  seq_mask = [float(i>0) for i in seq]\n",
        "  test_attention_masks.append(seq_mask) \n",
        "\n",
        "# Convert to tensors.\n",
        "test_inputs = torch.tensor(test_input_ids)\n",
        "test_masks = torch.tensor(test_attention_masks)\n",
        "test_labels = torch.tensor(test_labels)\n",
        "\n",
        "# Set the batch size.  \n",
        "batch_size = 32  \n",
        "\n",
        "# Create the DataLoader.\n",
        "test_data = TensorDataset(test_inputs, test_masks, test_labels)\n",
        "test_sampler = SequentialSampler(test_data)\n",
        "test_dataloader = DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "Truncation was not explicitely activated but `max_length` is provided a specific value, please use `truncation=True` to explicitely truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "DONE.\n",
            "\n",
            "       100 test comments\n",
            "        50 empethatic\n",
            "        50 not empetgatic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkRiAP11Oy6Q",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "4698a44a-71de-4bc5-b392-57c7c658e93c"
      },
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test sentences...'.format(len(test_inputs)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Measure elapsed time.\n",
        "t0 = time.time()\n",
        "\n",
        "# Predict \n",
        "for (step, batch) in enumerate(test_dataloader):\n",
        "    \n",
        "    # Add batch to GPU\n",
        "    batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "    # Progress update every 100 batches.\n",
        "    if step % 100 == 0 and not step == 0:\n",
        "        # Calculate elapsed time in minutes.\n",
        "        elapsed = format_time(time.time() - t0)\n",
        "        \n",
        "        # Report progress.\n",
        "        print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(test_dataloader), elapsed))\n",
        "\n",
        "\n",
        "    # Unpack the inputs from our dataloader\n",
        "    b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "    # Telling the model not to compute or store gradients, saving memory and \n",
        "    # speeding up prediction\n",
        "    with torch.no_grad():\n",
        "        # Forward pass, calculate logit predictions\n",
        "        outputs = model(b_input_ids, token_type_ids=None, \n",
        "                        attention_mask=b_input_mask)\n",
        "\n",
        "    logits = outputs[0]\n",
        "\n",
        "    # Move logits and labels to CPU\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "    label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "    # Store predictions and true labels\n",
        "    predictions.append(logits)\n",
        "    true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicting labels for 100 test sentences...\n",
            "    DONE.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_qYBDLEa3NL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Combine the results across the batches.\n",
        "predictions = np.concatenate(predictions, axis=0)\n",
        "true_labels = np.concatenate(true_labels, axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "smm5ku3-a9DE",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "dac93d23-917f-4b88-8305-d4140b6bf0b6"
      },
      "source": [
        "predictions[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2.7388322, -2.4836922],\n",
              "       [-3.2369266,  2.2707353],\n",
              "       [-2.9683049,  2.2026339],\n",
              "       [-1.7696551,  1.1304243],\n",
              "       [ 1.8712516, -1.8735001],\n",
              "       [ 2.9194505, -2.4704812],\n",
              "       [-2.7053072,  1.868814 ],\n",
              "       [-2.1090808,  1.3381222],\n",
              "       [-3.1315455,  2.2481697],\n",
              "       [ 3.183812 , -2.5147593]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QYywspV5a-vs",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "7f12eb3d-3660-414c-d6d0-000d12b41946"
      },
      "source": [
        "true_labels[0:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0, 1, 1, 1, 1, 0, 0, 1, 1, 0])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NtBGW7-abAcg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ae3e4a0e-b98d-4f12-9f95-0e65372f4b7c"
      },
      "source": [
        "# Our performance metric for the test set.\n",
        "from sklearn.metrics import roc_auc_score\n",
        "\n",
        "# Use the model output for label 1 as our predictions.\n",
        "p1 = predictions[:,1]\n",
        "\n",
        "# Calculate the ROC AUC.\n",
        "auc = roc_auc_score(true_labels, p1)\n",
        "\n",
        "print('Test ROC AUC: %.3f' %auc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test ROC AUC: 0.704\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4FL3-IVtbEmZ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 215
        },
        "outputId": "2addd324-8f00-44e0-c822-cca26112aaaa"
      },
      "source": [
        "import os\n",
        "\n",
        "# Saving best-practices: if you use defaults names for the model, you can reload it using from_pretrained()\n",
        "output_dir = './model_save/'\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "print(\"Saving model to %s\" % output_dir)\n",
        "\n",
        "# Save a trained model, configuration and tokenizer using `save_pretrained()`.\n",
        "# They can then be reloaded using `from_pretrained()`\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Take care of distributed/parallel training\n",
        "model_to_save.save_pretrained(output_dir)\n",
        "tokenizer.save_pretrained(output_dir)\n",
        "\n",
        "# Good practice: save your training arguments together with the trained model\n",
        "torch.save(args, os.path.join(output_dir, 'training_args.bin'))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Saving model to ./model_save/\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-3d65423a3e92>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m# Good practice: save your training arguments together with the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'training_args.bin'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aBaid-EEbKD_",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        },
        "outputId": "0c4e6226-c677-4ddb-ef94-64421064c281"
      },
      "source": [
        "# Mount Google Drive to this Notebook instance.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2sZlYfwtbN-U",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "gdrive_path = \"./drive/My Drive/BERT Classification Tutorial2/model_save/\"\n",
        "\n",
        "# Create output directory if needed\n",
        "if not os.path.exists(gdrive_path):\n",
        "    os.makedirs(gdrive_path)\n",
        "\n",
        "# Copy the model files to a directory in your Google Drive.\n",
        "!cp -r ./model_save/ \"./drive/My Drive/BERT Classification Tutorial2/model_save/\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cO4ZfZe7bbKt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}